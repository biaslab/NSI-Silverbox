{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silverbox\n",
    "\n",
    "Silverbox refers to one of the nonlinear system identification benchmarks on http://nonlinearbenchmark.org/#Silverbox. \n",
    "It is a simulation of a [Duffing oscillator](https://en.wikipedia.org/wiki/Duffing_equation), ocurring for instance in nonlinear spring pendulums.\n",
    "\n",
    "State-space model description of the system:\n",
    "\n",
    "$$\\begin{align}\n",
    "m \\frac{d^2 x(t)}{dt^2} + v \\frac{d x(t)}{dt} + a x(t) + b x^3(t) =&\\ u(t) + w(t) \\\\\n",
    "y(t) =&\\ x(t) + e(t)\n",
    "\\end{align}$$\n",
    "\n",
    "where\n",
    "$$\\begin{align}\n",
    "m     =&\\ \\text{mass} \\\\\n",
    "v     =&\\ \\text{viscous damping} \\\\\n",
    "a     =&\\ \\text{linear stiffness} \\\\\n",
    "b     =&\\ \\text{nonlinear stiffness} \\\\\n",
    "y(t)    =&\\ \\text{observation (displacement)} \\\\\n",
    "x(t)    =&\\ \\text{state (displacement)} \\\\\n",
    "u(t)    =&\\ \\text{force} \\\\\n",
    "e(t)    =&\\ \\text{measurement noise} \\\\\n",
    "w(t)    =&\\ \\text{process noise}\n",
    "\\end{align}$$\n",
    "\n",
    "The process noise is a Wiener process, where the increment is Gaussian distributed $w(t) = \\frac{d B(t)}{dt} \\sim \\mathcal{N}(0, \\tau^{-1}dt)$. The parameter $\\tau$ represents the precision of the process. The same holds for the measurement noise.\n",
    "\n",
    "## Solution steps\n",
    "\n",
    "### 1. Discretize\n",
    "\n",
    "I'm now using a central difference for the second derivative and a forward difference for the first derivative:\n",
    "\n",
    "$$\\begin{align}\n",
    "x''(t) \\approx&\\ \\frac{x(t+h) - 2x(t) + x(t-h)}{h^2} = \\frac{x_{t+1} - 2x_{t} + x_{t-1}}{(\\Delta t)^2}\\\\\n",
    "x'(t) \\approx&\\ \\frac{x(t+h) - x(t)}{h} = \\frac{x_{t+1} - x_{t}}{\\Delta t}\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\Delta t = t - (t-1) = 1$. Let $w_t$ be a sample from $\\mathcal{N}(0, \\tau^{-1})$. The DE can now be written as the following discrete-time system:\n",
    "\n",
    "$$\\begin{align}\n",
    "m (x_{t+1} - 2x_{t} + x_{t-1}) + v (x_{t+1} - x_{t}) + a x_t + b x_t^3 =&\\ u_t + w_t\n",
    "\\end{align}$$\n",
    "\n",
    "Re-writing this as a function of $x_{t+1}$ yields:\n",
    "$$\\begin{align}\n",
    "(m + v) x_{t+1}&\\ + (-2m - v + a) x_{t} + bx_t^3 + m x_{t-1} = u_t + w_t \\\\\n",
    "% x_t + \\frac{-2m - v}{m + v + a} x_{t-1} + \\frac{m}{m + v + a} x_{t-2} =&\\ \\frac{1}{m + v + a} u_t + \\frac{1}{m + v + a} w_t \\\\\n",
    "x_{t+1}&\\ = \\frac{2m + v - a}{m + v} x_{t} + \\frac{-b}{m + v}x_t^3 + \\frac{-m}{m + v} x_{t-1} + \\frac{1}{m + v} u_t + \\frac{1}{m + v} w_t \\, .\n",
    "\\end{align}$$\n",
    "\n",
    "### 2. Convert to multivariate first-order form\n",
    "\n",
    "I can cast the above system into matrix form:\n",
    "\n",
    "$$ \\underbrace{\\begin{bmatrix} x_{t+1} \\\\ x_{t} \\end{bmatrix}}_{z_t} = \\underbrace{\\begin{bmatrix} \\frac{2m+v-a}{m+v} - \\frac{b}{m+v}x_t^2 & \\frac{-m}{m+v} \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} x_{t} \\\\ x_{t-1} \\end{bmatrix}}_{A(\\theta, z_{t-1})} + \\begin{bmatrix} \\frac{1}{m+v} \\\\ 0 \\end{bmatrix} u_t + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\tilde{w}_t \\, ,$$\n",
    "\n",
    "where $A(\\theta, z_{t-1}) = Sz_{t-1} + c g(z_{t-1}, \\theta)$ with the familiar shifting operator $S$ and selection variable $c$, $g(z_{t-1}, \\theta) = \\theta_1 x_t + \\theta_2 x_t^3 + \\theta_3 x_{t-1}$ and\n",
    "\n",
    "$$\\begin{align}\n",
    "\\theta_1 =&\\ \\frac{2m+v-a}{m+v} \\\\\n",
    "\\theta_2 =&\\ \\frac{-b}{m+v} \\\\\n",
    "\\theta_3 =&\\ \\frac{-m}{m+v} \\, .\n",
    "\\end{align}$$\n",
    "\n",
    "Additionally, I'll introduce $\\eta = 1/(m+v)$. I can absorb $\\eta$ into $w_t$: $\\mathbb{V}[\\eta w_t] = \\eta^2 \\mathbb{V}[w_t] = \\eta^2 \\tau^{-1}$. I will rename $\\eta^2 \\tau^{-1}$ as $\\gamma^{-1}$ and let $\\tilde{w}_t \\sim \\mathcal{N}(0, \\gamma^{-1})$. In total, I have five unknowns $m,v,a,b,\\tau$ and five equations. We can solve the nonlinear system of equations to obtain estimates of the physical parameters from the shorthand parameters. The shorthands have the benefit that they allow for some freedom in choosing priors. The downside is that we lose the confidence estimate.\n",
    "\n",
    "The system is now a nonlinear autoregressive process:\n",
    "\n",
    "$$z_t = A(\\theta, z_{t-1}) + c\\eta u_t + \\tilde{w}_t$$\n",
    "\n",
    "Note that we need a two-dimensional state prior now (reminiscent of adding an initial condition on the velocity).\n",
    "\n",
    "### 3. Convert to Gaussian probability\n",
    "\n",
    "Integrating out $\\tilde{w}_t$ produces a Gaussian state transition node:\n",
    "\n",
    "$$z_t \\sim \\mathcal{N}(A(\\theta, z_{t-1}) + c\\eta u_t, V)$$\n",
    "\n",
    "where $V = \\begin{bmatrix} \\gamma^{-1} & 0 \\\\ 0 & \\epsilon \\end{bmatrix}$ and $V^{-1} = W = \\begin{bmatrix} \\gamma & 0 \\\\ 0 & 1/\\epsilon \\end{bmatrix}$.\n",
    "\n",
    "The observation likelihood maps to\n",
    "\n",
    "$$y_t \\sim \\mathcal{N}(c^{\\top} z_t, \\sigma^2)$$\n",
    "\n",
    "where $e_t \\sim \\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "### 4. Choose priors\n",
    "\n",
    "I will first study a situation with known measurement noise, i.e., where $\\sigma$ is fixed. The mass and stiffness are strictly positive parameters, but the viscous damping coefficient can be negative. As such, the $\\theta$'s and $\\eta$ can be both both positive and negative and modeled by Gaussian priors. Process precision $\\gamma$ is strictly positive, so we will use a gamma distribution:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\theta \\sim&\\ \\mathcal{N}(m^{0}_\\theta, v^{0}_\\theta) \\\\\n",
    "\\eta \\sim&\\ \\mathcal{N}(m^{0}_\\eta, v^{0}_\\eta) \\\\\n",
    "\\gamma \\sim&\\ \\Gamma(a^{0}_\\gamma, b^{0}_\\gamma) \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Let's first have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using CSV\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "\n",
    "viz = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file\n",
    "df = CSV.read(\"../data/SNLS80mV.csv\", ignoreemptylines=true)\n",
    "df = select(df, [:V1, :V2])\n",
    "\n",
    "# Shorthand\n",
    "input = df[:,1]\n",
    "output = df[:,2]\n",
    "\n",
    "# Time horizon\n",
    "T = size(df, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if viz\n",
    "    # Plot every n-th time-point to avoid figure size exploding\n",
    "    n = 10\n",
    "    p1 = Plots.scatter(1:n:T, output[1:n:T], color=\"black\", label=\"output\", markersize=2, size=(1600,800), xlabel=\"time (t)\", ylabel=\"response\")\n",
    "    # Plots.savefig(p1, \"viz/output_signal.png\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if viz\n",
    "    p2 = Plots.scatter(1:n:T, input[1:n:T], color=\"blue\", label=\"output\", markersize=2, size=(1600,800), xlabel=\"time (t)\", ylabel=\"control\")\n",
    "    # Plots.savefig(p2, \"viz/input_signal.png\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating parameters via Bayesian filtering\n",
    "\n",
    "Implementation with ForneyLab and AR node. The AR node is locally modified from the package LAR (LAR is in dev mode). It now contains an AutoregressiveControlNL node, where I can add a nonlinearity g as an argument.\n",
    "\n",
    "The major change from a linear AR to a nonlinear AR node is in working out the expectations for $g(x,\\theta)$ as opposed to $\\theta^{\\top}x$. There are a number of ways of doing this. I have chosen a first-order Taylor approximation:\n",
    "\n",
    "$$ g(x,\\theta) = g(m_x, m_{\\theta}) + J_{x}(m_x, m_{\\theta})^{\\top}(x - m_x) + J_{\\theta}(m_x, m_{\\theta})^{\\top}(\\theta - m_{\\theta}) \\, ,$$\n",
    "\n",
    "where $J_x$ denotes the partial derivative of $g$ with respect to $x$ and $J_{\\theta}$ w.r.t. $\\theta$. Note that our current $g$ is linear in $\\theta$ and you could argue that you don't need to approximate it. However, the first-order Taylor is exact in that case. The expectations are:\n",
    "\n",
    "#### First-order\n",
    "\n",
    "- W.r.t. both $x$ and $\\theta$:\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}_{q(x),q(\\theta)}[ g(x,\\theta)] =&\\ \\mathbb{E}_{q(x),q(\\theta)}[ g(m_x, m_{\\theta}) + J_{x}(m_x, m_{\\theta})^{\\top}(x - m_x) + J_{\\theta}(m_x, m_{\\theta})^{\\top}(\\theta - m_{\\theta}) ] \\\\\n",
    "=&\\ g(m_x, m_{\\theta}) + J_{x}(m_x, m_{\\theta})^{\\top}\\underbrace{\\mathbb{E}_{q(x)}[(x - m_x)]}_{0} + J_{\\theta}(m_x, m_{\\theta})^{\\top}\\underbrace{\\mathbb{E}_{q(\\theta)}[(\\theta - m_{\\theta})]}_{0} \\\\\n",
    "=&\\ g(m_x, m_{\\theta}) \\, .\n",
    "\\end{align}$$\n",
    "\n",
    "- W.r.t. $x$:\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}_{q(x)}[ g(x,\\theta)] =&\\ \\mathbb{E}_{q(x)}[ g(m_x, m_{\\theta}) + J_{x}(m_x, m_{\\theta})^{\\top}(x - m_x) + J_{\\theta}(m_x, m_{\\theta})^{\\top}(\\theta - m_{\\theta}) ] \\\\\n",
    "=&\\ g(m_x, m_{\\theta}) + J_{x}(m_x, m_{\\theta})^{\\top}\\underbrace{\\mathbb{E}_{q(x)}[(x - m_x)]}_{0} + J_{\\theta}(m_x, m_{\\theta})^{\\top}(\\theta - m_{\\theta}) \\\\\n",
    "=&\\ g(m_x, m_{\\theta}) + J_{\\theta}(m_x, m_{\\theta})^{\\top}(\\theta - m_{\\theta}) \\\\\n",
    "\\propto&\\ J_{\\theta}(m_x, m_{\\theta})^{\\top}\\theta \\, .\n",
    "\\end{align}$$\n",
    "\n",
    "- W.r.t. $\\theta$:\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}_{q(x),q(\\theta)}[ g(x,\\theta)] =&\\ \\mathbb{E}_{q(x),q(\\theta)}[ g(m_x, m_{\\theta}) + J_{x}(m_x, m_{\\theta})^{\\top}(x - m_x) + J_{\\theta}(m_x, m_{\\theta})^{\\top}(\\theta - m_{\\theta}) ] \\\\\n",
    "=&\\ g(m_x, m_{\\theta}) + J_{x}(m_x, m_{\\theta})^{\\top}(x - m_x) + J_{\\theta}(m_x, m_{\\theta})^{\\top}\\underbrace{\\mathbb{E}_{q(\\theta)}[(\\theta - m_{\\theta})]}_{0} \\\\\n",
    "=&\\ g(m_x, m_{\\theta}) + J_{x}(m_x, m_{\\theta})^{\\top}(x - m_x) \\\\\n",
    "\\propto&\\ J_{x}(m_x, m_{\\theta})^{\\top}x \\, .\n",
    "\\end{align}$$\n",
    "\n",
    "#### Second-order\n",
    "\n",
    "Shorthand: $g$ for $g(m_x, m_\\theta)$, $J_x$ for $J_x(m_x, m_{\\theta})$ and $J_{\\theta}(m_x, m_{\\theta})$.\n",
    "\n",
    "- W.r.t. both $x$ and $\\theta$:\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}_{q(x),q(\\theta)}[ g(x,\\theta)^2] =&\\ \\mathbb{E}_{q(x),q(\\theta)}[ \\big( g + J_{x}^{\\top}(x - m_x) + J_{\\theta}^{\\top}(\\theta - m_{\\theta}) \\big)^2] \\\\\n",
    "=&\\ g^{2} + 2 g J_{x}^{\\top}\\underbrace{\\mathbb{E}_{q(x)}[(x - m_x)]}_{0}  + 2 g J_{\\theta}^{\\top}\\underbrace{\\mathbb{E}_{q(\\theta)}[(\\theta - m_{\\theta})]}_{0} + 2 J_{x}^{\\top}\\underbrace{\\mathbb{E}_{q(x)}[(x - m_x)]}_{0} J_{\\theta}^{\\top}\\underbrace{\\mathbb{E}_{q(\\theta)}[(\\theta - m_{\\theta})]}_{0} \\\\\n",
    "&\\ + J_x^{\\top}\\mathbb{E}_{q(x)}[(x - m_x)(x - m_x)^{\\top}]J_x + J_{\\theta}^{\\top}\\mathbb{E}_{q(\\theta)}[(\\theta - m_{\\theta})(\\theta - m_{\\theta})^{\\top}] J_{\\theta} \\\\\n",
    "=&\\ g^{2} + J_x^{\\top} V_x J_x + J_{\\theta}^{\\top} V_{\\theta} J_{\\theta} \\, .\n",
    "\\end{align}$$\n",
    "\n",
    "- W.r.t. $x$:\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}_{q(x)}[ g(x,\\theta)^2] =&\\ \\mathbb{E}_{q(x)}[ \\big( g + J_{x}^{\\top}(x - m_x) + J_{\\theta}^{\\top}(\\theta - m_{\\theta}) \\big)^2] \\\\\n",
    "=&\\ g^{2} + 2 g J_{x}^{\\top}\\underbrace{\\mathbb{E}_{q(x)}[(x - m_x)]}_{0}  + 2 g J_{\\theta}^{\\top}(\\theta - m_{\\theta}) + 2 J_{x}^{\\top}\\underbrace{\\mathbb{E}_{q(x)}[(x - m_x)]}_{0} J_{\\theta}^{\\top}(\\theta - m_{\\theta}) \\\\\n",
    "&\\ + J_x^{\\top}\\mathbb{E}_{q(x)}[(x - m_x)(x - m_x)^{\\top}]J_x + J_{\\theta}^{\\top}(\\theta - m_{\\theta})(\\theta - m_{\\theta})^{\\top} J_{\\theta} \\\\\n",
    "=&\\ g^{2} + 2 g J_{\\theta}^{\\top}(\\theta - m_{\\theta}) + J_{\\theta}^{\\top} V_{\\theta} J_{\\theta} + J_x^{\\top} V_x J_x \\\\\n",
    "\\propto&\\ (g-J_{\\theta}^{\\top}m_{\\theta}) J_{\\theta}^{\\top}\\theta + \\theta^{\\top}J_{\\theta}(g - J_{\\theta}^{\\top}m_{\\theta}) + \\theta^{\\top} J_{\\theta} J_{\\theta}^{\\top} \\theta \\, .\n",
    "\\end{align}$$\n",
    "\n",
    "- W.r.t. $\\theta$:\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}_{q(\\theta)}[ g(x,\\theta)^2] =&\\ \\mathbb{E}_{q(\\theta)}[ \\big( g + J_{x}^{\\top}(x - m_x) + J_{\\theta}^{\\top}(\\theta - m_{\\theta}) \\big)\\big(\\dots \\big)^{\\top}] \\\\\n",
    "=&\\ g^{2} + 2 g J_{x}^{\\top}(x - m_x)  + 2 g J_{\\theta}^{\\top}\\underbrace{\\mathbb{E}_{q(\\theta)}[(\\theta - m_{\\theta})]}_{0} + 2 J_{x}^{\\top}(x - m_x) J_{\\theta}^{\\top}\\underbrace{\\mathbb{E}_{q(\\theta)}[(\\theta - m_{\\theta})]}_{0} \\\\\n",
    "&\\ + J_x^{\\top}(x - m_x)(x - m_x)^{\\top}J_x + J_{\\theta}^{\\top}\\mathbb{E}_{q(\\theta)}[(\\theta - m_{\\theta})(\\theta - m_{\\theta})^{\\top}] J_{\\theta} \\\\\n",
    "=&\\ g^{2} + 2 g J_{x}^{\\top}(x - m_x) + J_x^{\\top} (x - m_x)(x - m_x)^{\\top} J_x + J_{\\theta}^{\\top} V_{\\theta} J_{\\theta} \\\\\n",
    "\\propto&\\ (g-J_x^{\\top}m_x) J_x^{\\top}x + x^{\\top}J_x(g - J_x^{\\top}m_x) + x^{\\top} J_x J_x^{\\top} x \\, .\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForneyLab\n",
    "using ForneyLab: unsafeMean, unsafeCov, unsafeVar, unsafePrecision\n",
    "using LAR\n",
    "using LAR.Node, LAR.Data\n",
    "using ProgressMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start graph\n",
    "graph = FactorGraph()\n",
    "\n",
    "# Static parameters\n",
    "@RV θ ~ GaussianMeanPrecision(placeholder(:m_θ, dims=(2,)), placeholder(:w_θ, dims=(2,2)))\n",
    "@RV η ~ GaussianMeanPrecision(placeholder(:m_η), placeholder(:w_η))\n",
    "@RV γ ~ Gamma(placeholder(:a_γ), placeholder(:b_γ))\n",
    "\n",
    "# Observation selection variable\n",
    "c = [1, 0]\n",
    "\n",
    "# Measurement precision\n",
    "σ = 1e4\n",
    "\n",
    "# State prior\n",
    "@RV z_t ~ GaussianMeanPrecision(placeholder(:m_z, dims=(2,)), placeholder(:w_z, dims=(2, 2)), id=:z_t)\n",
    "\n",
    "# Autoregressive node\n",
    "@RV x_t ~ AutoregressiveControl(θ, z_t, η, placeholder(:u_t), γ, id=:x_t)\n",
    "\n",
    "# Specify likelihood\n",
    "@RV y_t ~ GaussianMeanPrecision(dot(c, x_t), σ, id=:y_t)\n",
    "\n",
    "# Placeholder for observation\n",
    "placeholder(y_t, :y_t)\n",
    "\n",
    "# Draw time-slice subgraph\n",
    "ForneyLab.draw(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer an algorithm\n",
    "q = PosteriorFactorization(z_t, x_t, θ, η, γ, ids=[:z, :x, :θ, :η, :γ])\n",
    "algo = variationalAlgorithm(q, free_energy=false)\n",
    "source_code = algorithmSourceCode(algo, free_energy=false)\n",
    "eval(Meta.parse(source_code));\n",
    "# println(source_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at only the first few timepoints\n",
    "# T = 100\n",
    "T = size(df, 1);\n",
    "\n",
    "# Inference parameters\n",
    "num_iterations = 10\n",
    "\n",
    "# Initialize marginal distribution and observed data dictionaries\n",
    "data = Dict()\n",
    "marginals = Dict()\n",
    "\n",
    "# Initialize arrays of parameterizations\n",
    "params_x = (zeros(2,T+1), repeat(.1 .*float(eye(2)), outer=(1,1,T+1)))\n",
    "params_θ = (ones(2,T+1), repeat(.1 .*float(eye(2)), outer=(1,1,T+1)))\n",
    "params_η = (ones(1,T+1), 0.1*ones(1,T+1))\n",
    "params_γ = (0.1*ones(1,T+1), 0.01*ones(1,T+1))\n",
    "\n",
    "# Start progress bar\n",
    "p = Progress(T, 1, \"At time \")\n",
    "\n",
    "# Perform inference at each time-step\n",
    "for t = 1:T\n",
    "\n",
    "    # Update progress bar\n",
    "    update!(p, t)\n",
    "\n",
    "    # Initialize marginals\n",
    "    marginals[:x_t] = ProbabilityDistribution(Multivariate, GaussianMeanPrecision, m=params_x[1][:,t], w=params_x[2][:,:,t])\n",
    "    marginals[:z_t] = ProbabilityDistribution(Multivariate, GaussianMeanPrecision, m=params_x[1][:,t], w=params_x[2][:,:,t])\n",
    "    marginals[:θ] = ProbabilityDistribution(Multivariate, GaussianMeanPrecision, m=params_θ[1][:,t], w=params_θ[2][:,:,t])\n",
    "    marginals[:η] = ProbabilityDistribution(Univariate, GaussianMeanPrecision, m=params_η[1][1,t], w=params_η[2][1,t])\n",
    "    marginals[:γ] = ProbabilityDistribution(Univariate, Gamma, a=params_γ[1][1,t], b=params_γ[2][1,t])\n",
    "    \n",
    "    data = Dict(:y_t => output[t],\n",
    "                :u_t => input[t],\n",
    "                :m_z => params_x[1][:,t],\n",
    "                :w_z => params_x[2][:,:,t],\n",
    "                :m_θ => params_θ[1][:,t],\n",
    "                :w_θ => params_θ[2][:,:,t],\n",
    "                :m_η => params_η[1][1,t],\n",
    "                :w_η => params_η[2][1,t],\n",
    "                :a_γ => params_γ[1][1,t],\n",
    "                :b_γ => params_γ[2][1,t])\n",
    "\n",
    "    # Iterate variational parameter updates\n",
    "    for i = 1:num_iterations\n",
    "\n",
    "        stepx!(data, marginals)\n",
    "        stepθ!(data, marginals)\n",
    "        stepη!(data, marginals)\n",
    "        stepγ!(data, marginals)\n",
    "    end\n",
    "\n",
    "    # Store current parameterizations of marginals\n",
    "    params_x[1][:,t+1] = unsafeMean(marginals[:x_t])\n",
    "    params_x[2][:,:,t+1] = marginals[:x_t].params[:w]\n",
    "    params_θ[1][:,t+1] = unsafeMean(marginals[:θ])\n",
    "    params_θ[2][:,:,t+1] = marginals[:θ].params[:w]\n",
    "    params_η[1][1,t+1] = unsafeMean(marginals[:η])\n",
    "    params_η[2][1,t+1] = marginals[:η].params[:w]\n",
    "    params_γ[1][1,t+1] = marginals[:γ].params[:a]\n",
    "    params_γ[2][1,t+1] = marginals[:γ].params[:b]\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean of state marginals\n",
    "estimated_states = params_x[1][1,2:end]\n",
    "\n",
    "if viz\n",
    "    # Plot every n-th time-point to avoid figure size exploding\n",
    "    n = 10\n",
    "    p1 = Plots.scatter(1:n:T, output[1:n:T], color=\"black\", label=\"output\", markersize=2, size=(1400,600), xlabel=\"time (t)\", ylabel=\"response\")\n",
    "    Plots.plot!(1:n:T, estimated_states[1:n:T], color=\"red\", linewidth=1, label=\"estimated\")\n",
    "#     Plots.savefig(p1, \"viz/estimated_states01.png\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean of coefficient marginals\n",
    "estimated_coeffs_1_mean = params_θ[1][1,2:end]\n",
    "estimated_coeffs_1_std = sqrt.(inv.(params_θ[2][1,1,2:end]))\n",
    "estimated_coeffs_2_mean = params_θ[1][2,2:end]\n",
    "estimated_coeffs_2_std = sqrt.(inv.(params_θ[2][2,2,2:end]))\n",
    "\n",
    "if viz\n",
    "    \n",
    "#     # Plot both coefficients within the same figure\n",
    "#     Plots.plot(1:n:T, estimated_coeffs_1_mean[1:n:T], ribbon=[estimated_coeffs_1_std[1:n:T], estimated_coeffs_1_std[1:n:T]], color=\"red\", label=\"θ_1\", xlabel=\"time (t)\", ylim=[-1.5, 1.5])\n",
    "#     Plots.plot!(1:n:T, estimated_coeffs_2_mean[1:n:T], ribbon=[estimated_coeffs_2_std[1:n:T], estimated_coeffs_2_std[1:n:T]], color=\"blue\", label=\"θ_2\")\n",
    "# #     Plots.savefig(\"viz/estimated_coeffs.png\")\n",
    "    \n",
    "    # Plot both coefficients next to each other\n",
    "    p2a = Plots.plot(1:n:T, estimated_coeffs_1_mean[1:n:T], ribbon=[estimated_coeffs_1_std[1:n:T], estimated_coeffs_1_std[1:n:T]], color=\"red\", label=\"θ_1\", xlabel=\"time (t)\")\n",
    "    p2b = Plots.plot(1:n:T, estimated_coeffs_2_mean[1:n:T], ribbon=[estimated_coeffs_2_std[1:n:T], estimated_coeffs_2_std[1:n:T]], color=\"blue\", label=\"θ_2\", xlabel=\"time (t)\")\n",
    "    p2 = plot(p2a, p2b, size=(1200,400))\n",
    "#     Plots.savefig(p2, \"viz/estimated_coeffs_subp.png\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean of control coefficient marginals\n",
    "estimated_ccoeff_mean = params_η[1][1,2:end]\n",
    "estimated_ccoeff_std = sqrt.(inv.(params_η[2][1,2:end]))\n",
    "\n",
    "if viz\n",
    "    # Plot both coefficients next to each other\n",
    "    p3 = Plots.plot(1:n:T, estimated_ccoeff_mean[1:n:T], ribbon=[estimated_ccoeff_std[1:n:T], estimated_ccoeff_std[1:n:T]], color=\"blue\", label=\"η\", xlabel=\"time (t)\", size=(800,600), ylim=[0.0, 0.25])\n",
    "#     Plots.savefig(p3, \"viz/estimated_ccoeff.png\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean of process precision marginals\n",
    "estimated_pnoise_mean = params_γ[1][1,2:end] ./ params_γ[2][1,2:end]\n",
    "estimated_pnoise_std = sqrt.(params_γ[1][1,2:end] ./ params_γ[2][1,2:end].^2)\n",
    "\n",
    "if viz\n",
    "    # Plot both coefficients next to each other\n",
    "    p4 = Plots.plot(1:n:T, estimated_pnoise_mean[1:n:T], ribbon=[estimated_pnoise_std[1:n:T], estimated_pnoise_std[1:n:T]],color=\"blue\", label=\"γ\", xlabel=\"time (t)\")\n",
    "#     Plots.savefig(p4, \"viz/estimated_pnoise.png\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving nonlinear system of equations\n",
    "\n",
    "We currently have estimates for $\\theta_1$, $\\theta_2$, and $\\eta$. But we want to know the original coefficients, $m$, $v$ and $a$, which actually have a physical meaning. To obtain estimates for those, we have to solve the following nonlinear system of equations:\n",
    "\n",
    "$$\\begin{align} \n",
    "\\hat{\\theta}_1 =&\\ \\frac{2m + v}{m + v + a} \\\\\n",
    "\\hat{\\theta}_2 =&\\ \\frac{-m}{m + v + a} \\\\\n",
    "\\hat{\\eta} =&\\ \\frac{1}{m + v + a} \\\\\n",
    "\\hat{\\gamma}^{-1} =&\\ \\frac{1}{\\tau (m + v + a)^2}\n",
    "\\end{align}$$\n",
    "\n",
    "Implementation using NLsolve.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current estimates of parameters\n",
    "global estimates = [estimated_coeffs_1_mean[end], estimated_coeffs_2_mean[end], estimated_ccoeff_mean[end], inv(estimated_pnoise_mean[end])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nonlinear system of equations\n",
    "function F!(F, x)\n",
    "    F[1] = (2*x[1] + x[2])/(x[1] + x[2] + x[3])  - estimates[1]\n",
    "    F[2] = (-x[1])/(x[1] + x[2] + x[3]) - estimates[2]\n",
    "    F[3] = 1/(x[1] + x[2] + x[3]) - estimates[3]\n",
    "    F[4] = 1/(x[4]*(x[1] + x[2] + x[3])^2) - estimates[4] \n",
    "end\n",
    "\n",
    "# Jacobian of each equation\n",
    "function J!(J, x)\n",
    "    \n",
    "    # F[1]\n",
    "    J[1, 1] = (x[2] + 2*x[3])/(x[1] + x[2] + x[3])^2\n",
    "    J[1, 2] = (x[3] - x[1])/(x[1] + x[2] + x[3])^2\n",
    "    J[1, 3] = (-2*x[1] - x[2])/(x[1] + x[2] + x[3])^2\n",
    "    J[1, 4] = 0.\n",
    "    \n",
    "    # F[2]\n",
    "    J[2, 1] = (x[2] + x[3])/(x[1] + x[2] + x[3])^2\n",
    "    J[2, 2] = x[1]/(x[1] + x[2] + x[3])^2\n",
    "    J[2, 3] = x[1]/(x[1] + x[2] + x[3])^2\n",
    "    J[2, 4] = 0.\n",
    "    \n",
    "    # F[3]\n",
    "    J[3, 1] = -1/(x[1] + x[2] + x[3])^2\n",
    "    J[3, 2] = -1/(x[1] + x[2] + x[3])^2\n",
    "    J[3, 3] = -1/(x[1] + x[2] + x[3])^2\n",
    "    J[3, 4] = 0.\n",
    "    \n",
    "    # F[4]\n",
    "    J[4, 1] = -1/(2*x[4]*(x[1] + x[2] + x[3])^(3/2))\n",
    "    J[4, 2] = -1/(2*x[4]*(x[1] + x[2] + x[3])^(3/2))\n",
    "    J[4, 3] = -1/(2*x[4]*(x[1] + x[2] + x[3])^(3/2))\n",
    "    J[4, 4] = -1/(x[4]^2*sqrt(x[1] + x[2] + x[3]))\n",
    "    \n",
    "end\n",
    "\n",
    "# Call solver\n",
    "# x_solved = nlsolve(F!, J!, [1. 1. 1. 1.])\n",
    "x_solved = nlsolve(F!, [1. 1. 1. 1.], autodiff=:forward)\n",
    "\n",
    "# Extract new estimates\n",
    "global m, v, a, τ = x_solved.zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in total we estimate the dynamical parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"m = \" *string(m))\n",
    "println(\"v = \" *string(v))\n",
    "println(\"a = \" *string(a))\n",
    "println(\"τ = \" *string(τ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm going to estimate the dynamical parameters for the entire trajectory. Note that I could do this at inference time as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = zeros(T,1)\n",
    "v = zeros(T,1)\n",
    "a = zeros(T,1)\n",
    "τ = zeros(T,1)\n",
    "\n",
    "# Extract means of marginals\n",
    "estimated_θ1 = params_θ[1][1,2:end]\n",
    "estimated_θ2 = params_θ[1][2,2:end]\n",
    "estimated_η = params_η[1][1,2:end]\n",
    "estimated_γ = inv.(params_γ[1][1,2:end] ./ params_γ[2][1,2:end])\n",
    "\n",
    "for t = 1:T\n",
    "    \n",
    "    function F!(F, x)\n",
    "        F[1] = (2*x[1] + x[2])/(x[1] + x[2] + x[3])  - estimated_θ1[t]\n",
    "        F[2] = (-x[1])/(x[1] + x[2] + x[3]) - estimated_θ2[t]\n",
    "        F[3] = 1/(x[1] + x[2] + x[3]) - estimated_η[t]\n",
    "        F[4] = 1/(x[4]*(x[1] + x[2] + x[3])^2) - estimated_γ[t] \n",
    "    end\n",
    "    \n",
    "    # Call solver\n",
    "#     x_solved = nlsolve(F!, J!, [1. 1. 1. 1.])\n",
    "    x_solved = nlsolve(F!, [0.1 0.1 0.1 0.1], autodiff=:forward)\n",
    "\n",
    "    # Extract new estimates\n",
    "    m[t], v[t], a[t], τ[t] = x_solved.zero\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot belief evolution for mass\n",
    "Plots.plot(1:n:T, m[1:n:T], color=\"red\", label=\"m\", xlabel=\"time (t)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot belief evolution for friction coefficient\n",
    "Plots.plot(1:n:T, v[1:n:T], color=\"blue\", label=\"v\", xlabel=\"time (t)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot belief evolution for linear stiffness\n",
    "Plots.plot(1:n:T, a[1:n:T], color=\"green\", label=\"a\", xlabel=\"time (t)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot belief evolution for process precision\n",
    "Plots.plot(1:n:T, τ[1:n:T], color=\"purple\", label=\"τ\", xlabel=\"time (t)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
